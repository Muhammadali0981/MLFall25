{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 05 - Ensemble Learning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "import xgboost as xgb\n",
                "from mlxtend.evaluate import bias_variance_decomp # Try to import, might need installation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 1: Ensemble Pipeline (Heart Dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "df = pd.read_csv(r\"C:\\Ali\\Programming\\MLFall25\\Lab05\\heart.csv\")\n",
                "\n",
                "# EDA\n",
                "print(\"--- EDA ---\")\n",
                "print(\"Missing Values:\", df.isnull().sum().sum())\n",
                "print(\"Duplicates:\", df.duplicated().sum())\n",
                "if df.duplicated().sum() > 0:\n",
                "    df = df.drop_duplicates()\n",
                "\n",
                "target = 'target' # Assuming 'target' is the label based on standard heart.csv, check if it's 'Label'\n",
                "if 'Label' in df.columns:\n",
                "    target = 'Label'\n",
                "elif 'target' not in df.columns:\n",
                "    # Fallback or check columns\n",
                "    print(\"Columns:\", df.columns)\n",
                "    target = df.columns[-1]\n",
                "\n",
                "print(f\"Target Variable: {target}\")\n",
                "print(\"Target Balance:\")\n",
                "print(df[target].value_counts())\n",
                "\n",
                "# Scaling\n",
                "X = df.drop(columns=[target])\n",
                "y = df[target]\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "# Split: Train 80%, Test 20%\n",
                "X_train_full, X_test, y_train_full, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)\n",
                "\n",
                "# Validation Split: Train 70%, Val 30% (from Train split)\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.3, random_state=0)\n",
                "\n",
                "print(f\"Split Sizes: Train={X_train.shape[0]}, Val={X_val.shape[0]}, Test={X_test.shape[0]}\")\n",
                "\n",
                "# Models\n",
                "rf = RandomForestClassifier(random_state=0)\n",
                "xgb_model = xgb.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss')\n",
                "ada = AdaBoostClassifier(random_state=0, algorithm='SAMME')\n",
                "\n",
                "models = {'Random Forest': rf, 'XGBoost': xgb_model, 'AdaBoost': ada}\n",
                "\n",
                "print(\"\\n--- Model Comparison ---\")\n",
                "for name, model in models.items():\n",
                "    model.fit(X_train, y_train)\n",
                "    train_acc = model.score(X_train, y_train)\n",
                "    test_acc = model.score(X_test, y_test)\n",
                "    print(f\"{name}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 2: Voting Classifier (restecg, oldpeak)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract Attributes\n",
                "features_t2 = ['restecg', 'oldpeak']\n",
                "X_t2 = df[features_t2]\n",
                "y_t2 = df[target]\n",
                "\n",
                "# Split (using same random state for consistency)\n",
                "X_train_t2, X_test_t2, y_train_t2, y_test_t2 = train_test_split(X_t2, y_t2, test_size=0.2, random_state=0)\n",
                "\n",
                "# Models\n",
                "dt = DecisionTreeClassifier(random_state=0)\n",
                "knn = KNeighborsClassifier()\n",
                "rf = RandomForestClassifier(random_state=0)\n",
                "xgb_model = xgb.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss')\n",
                "\n",
                "estimators = [('dt', dt), ('knn', knn), ('rf', rf), ('xgb', xgb_model)]\n",
                "\n",
                "# Check Voting Parameter (Hard vs Soft)\n",
                "print(\"\\n--- Voting Classifier (Hard vs Soft) ---\")\n",
                "for voting_type in ['hard', 'soft']:\n",
                "    vc = VotingClassifier(estimators=estimators, voting=voting_type)\n",
                "    vc.fit(X_train_t2, y_train_t2)\n",
                "    acc = vc.score(X_test_t2, y_test_t2)\n",
                "    print(f\"Voting='{voting_type}': Accuracy = {acc:.4f}\")\n",
                "\n",
                "# Check Best Weights (Simple manual check or heuristic)\n",
                "# For demonstration, we'll try equal weights vs some variations\n",
                "print(\"\\n--- Weight Optimization (Heuristic) ---\")\n",
                "weights_list = [\n",
                "    [1, 1, 1, 1], # Equal\n",
                "    [1, 1, 2, 2], # Favor stronger models (RF, XGB)\n",
                "    [1, 2, 1, 1], # Favor KNN\n",
                "    [2, 1, 1, 1]  # Favor DT\n",
                "]\n",
                "\n",
                "best_acc = 0\n",
                "best_weights = None\n",
                "\n",
                "for w in weights_list:\n",
                "    vc = VotingClassifier(estimators=estimators, voting='soft', weights=w)\n",
                "    vc.fit(X_train_t2, y_train_t2)\n",
                "    acc = vc.score(X_test_t2, y_test_t2)\n",
                "    print(f\"Weights={w}: Accuracy = {acc:.4f}\")\n",
                "    if acc > best_acc:\n",
                "        best_acc = acc\n",
                "        best_weights = w\n",
                "\n",
                "print(f\"Best Weights: {best_weights} with Accuracy: {best_acc:.4f}\")\n",
                "\n",
                "# Bias-Variance Tradeoff Graph\n",
                "# Using mlxtend if available, otherwise manual explanation\n",
                "try:\n",
                "    print(\"\\nCalculating Bias-Variance Decomposition...\")\n",
                "    # Note: bias_variance_decomp requires integer labels and numpy arrays\n",
                "    X_train_np = X_train_t2.values\n",
                "    X_test_np = X_test_t2.values\n",
                "    y_train_np = y_train_t2.values\n",
                "    y_test_np = y_test_t2.values\n",
                "    \n",
                "    vc_best = VotingClassifier(estimators=estimators, voting='soft', weights=best_weights)\n",
                "    \n",
                "    mse, bias, var = bias_variance_decomp(vc_best, X_train_np, y_train_np, X_test_np, y_test_np, loss='mse', num_rounds=200, random_seed=0)\n",
                "    \n",
                "    print(f'MSE: {mse:.4f}')\n",
                "    print(f'Bias: {bias:.4f}')\n",
                "    print(f'Variance: {var:.4f}')\n",
                "    \n",
                "    # Plotting\n",
                "    plt.figure(figsize=(8, 5))\n",
                "    plt.bar(['MSE', 'Bias', 'Variance'], [mse, bias, var], color=['blue', 'orange', 'green'])\n",
                "    plt.title('Bias-Variance Tradeoff (Voting Classifier)')\n",
                "    plt.ylabel('Error')\n",
                "    plt.show()\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"mlxtend not installed. Skipping Bias-Variance calculation.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error in Bias-Variance decomposition: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 3: Voting Classifier (restecg, chol)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract Attributes\n",
                "features_t3 = ['restecg', 'chol']\n",
                "X_t3 = df[features_t3]\n",
                "y_t3 = df[target]\n",
                "\n",
                "# Split\n",
                "X_train_t3, X_test_t3, y_train_t3, y_test_t3 = train_test_split(X_t3, y_t3, test_size=0.2, random_state=0)\n",
                "\n",
                "# Models\n",
                "rf = RandomForestClassifier(random_state=0)\n",
                "ada = AdaBoostClassifier(random_state=0, algorithm='SAMME')\n",
                "xgb_model = xgb.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss') # Added for comparison as per prompt implication\n",
                "\n",
                "# Voting Classifier (RF + AdaBoost)\n",
                "vc_t3 = VotingClassifier(estimators=[('rf', rf), ('ada', ada)], voting='soft')\n",
                "\n",
                "# Train and Collect Accuracies\n",
                "models_t3 = [rf, ada, xgb_model, vc_t3]\n",
                "names_t3 = ['Random Forest', 'AdaBoost', 'XGBoost', 'Voting (RF+Ada)']\n",
                "train_accs = []\n",
                "test_accs = []\n",
                "\n",
                "for model in models_t3:\n",
                "    model.fit(X_train_t3, y_train_t3)\n",
                "    train_accs.append(model.score(X_train_t3, y_train_t3))\n",
                "    test_accs.append(model.score(X_test_t3, y_test_t3))\n",
                "\n",
                "# Plotting\n",
                "x = np.arange(len(names_t3))\n",
                "width = 0.35\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "rects1 = ax.bar(x - width/2, train_accs, width, label='Train Accuracy')\n",
                "rects2 = ax.bar(x + width/2, test_accs, width, label='Test Accuracy')\n",
                "\n",
                "ax.set_ylabel('Accuracy')\n",
                "ax.set_title('Accuracy Comparison: Individual vs Ensemble')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(names_t3)\n",
                "ax.legend()\n",
                "\n",
                "ax.bar_label(rects1, padding=3, fmt='%.2f')\n",
                "ax.bar_label(rects2, padding=3, fmt='%.2f')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}